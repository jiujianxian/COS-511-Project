@article{robbins1952,
author = "Robbins, Herbert",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "09",
number = "5",
pages = "527--535",
publisher = "American Mathematical Society",
title = "Some aspects of the sequential design of experiments",
url = "https://projecteuclid.org:443/euclid.bams/1183517370",
volume = "58",
year = "1952"
}

@article{thompson1933,
ISSN = {00063444},
URL = {http://www.jstor.org/stable/2332286},
author = {William R. Thompson},
journal = {Biometrika},
number = {3/4},
pages = {285--294},
publisher = {[Oxford University Press, Biometrika Trust]},
title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
volume = {25},
year = {1933}
}

@article{gittins1979,
ISSN = {00359246},
URL = {http://www.jstor.org/stable/2985029},
abstract = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
author = {J. C. Gittins},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
number = {2},
pages = {148--177},
publisher = {[Royal Statistical Society, Wiley]},
title = {Bandit Processes and Dynamic Allocation Indices},
volume = {41},
year = {1979}
}

@article{bubeck2012,
url = {http://dx.doi.org/10.1561/2200000024},
year = {2012},
volume = {5},
journal = {Foundations and Trends in Machine Learning},
title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
doi = {10.1561/2200000024},
issn = {1935-8237},
number = {1},
pages = {1-122},
author = {Sébastien Bubeck and Nicolò Cesa-Bianchi}
}

@misc{lecture16,
title={COS 511: Lecture Notes 16}, url={https://www.cs.princeton.edu/courses/archive/spring19/cos511/scribe_notes/0403.pdf},
publisher={Princeton University},
author={Mong, Arnold K.},
year={2019},
month={Apr}
}

@misc{lecture18,
title={COS 511: Lecture Notes 18}, url={https://www.cs.princeton.edu/courses/archive/spring19/cos511/scribe_notes/0410.pdf},
publisher={Princeton University},
author={Sokolova, Ksenia},
year={2019},
month={Apr}
}

@InProceedings{mohri2005,
author="Vermorel, Joann{\`e}s
and Mohri, Mehryar",
editor="Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
title="Multi-armed Bandit Algorithms and Empirical Evaluation",
booktitle="Machine Learning: ECML 2005",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="437--448",
abstract="The multi-armed bandit problem for a gambler is to decide which arm of a K-slot machine to pull to maximize his total reward in a series of trials. Many real-world learning and optimization problems can be modeled in this way. Several strategies or algorithms have been proposed as a solution to this problem in the last two decades, but, to our knowledge, there has been no common evaluation of these algorithms.",
isbn="978-3-540-31692-3"
}

@book{sutton1998,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Introduction to Reinforcement Learning},
year = {1998},
isbn = {0262193981},
edition = {1st},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
}

@inproceedings{evendar2002,
author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
title = {PAC Bounds for Multi-armed Bandit and Markov Decision Processes},
booktitle = {Proceedings of the 15th Annual Conference on Computational Learning Theory},
series = {COLT '02},
year = {2002},
isbn = {3-540-43836-X},
pages = {255--270},
numpages = {16},
url = {http://dl.acm.org/citation.cfm?id=648301.755490},
acmid = {755490},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
}

@article{auer2002,
author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul},
title = {Finite-time Analysis of the Multiarmed Bandit Problem},
journal = {Mach. Learn.},
issue_date = {May-June 2002},
volume = {47},
number = {2-3},
month = may,
year = {2002},
issn = {0885-6125},
pages = {235--256},
numpages = {22},
url = {https://doi.org/10.1023/A:1013689704352},
doi = {10.1023/A:1013689704352},
acmid = {599677},
publisher = {Kluwer Academic Publishers},
address = {Hingham, MA, USA},
keywords = {adaptive allocation rules, bandit problems, finite horizon regret},
}

@misc{lecture8,
title={COS 511: Lecture Notes 8}, url={https://www.cs.princeton.edu/courses/archive/spring19/cos511/scribe_notes/0227.pdf},
publisher={Princeton University},
author={Li, Haochen},
year={2019},
month={Apr}
}

@article{lai1985,
title = "Asymptotically efficient adaptive allocation rules",
journal = "Advances in Applied Mathematics",
volume = "6",
number = "1",
pages = "4 - 22",
year = "1985",
issn = "0196-8858",
doi = "https://doi.org/10.1016/0196-8858(85)90002-8",
url = "http://www.sciencedirect.com/science/article/pii/0196885885900028",
author = "T.L Lai and Herbert Robbins"
}

@article{auer2003,
author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Schapire, Robert E.},
title = {The Nonstochastic Multiarmed Bandit Problem},
journal = {SIAM J. Comput.},
issue_date = {2003},
volume = {32},
number = {1},
month = jan,
year = {2003},
issn = {0097-5397},
pages = {48--77},
numpages = {30},
url = {https://doi.org/10.1137/S0097539701398375},
doi = {10.1137/S0097539701398375},
acmid = {589365},
publisher = {Society for Industrial and Applied Mathematics},
address = {Philadelphia, PA, USA},
keywords = {adversarial bandit problem, unknown matrix games},
}

@misc{lecture21,
title={COS 511: Lecture Notes 21}, url={https://www.cs.princeton.edu/courses/archive/spring19/cos511/scribe_notes/0422.pdf},
publisher={Princeton University},
author={Wan, Xin},
year={2019},
month={Apr}
}

@article{russo2018,
url = {http://dx.doi.org/10.1561/2200000070},
year = {2018},
volume = {11},
journal = {Foundations and Trends in Machine Learning},
title = {A Tutorial on Thompson Sampling},
doi = {10.1561/2200000070},
issn = {1935-8237},
number = {1},
pages = {1-96},
author = {Daniel J. Russo and Benjamin Van Roy and Abbas Kazerouni and Ian Osband and Zheng Wen}
}

@article{agrawal2012,
author = {Shipra Agrawal and Navin Goyal},
title = {Further Optimal Regret Bounds for Thompson Sampling},
journal = {CoRR},
volume = {abs/1209.3353},
year = {2012},
url = {http://arxiv.org/abs/1209.3353},
archivePrefix = {arXiv},
eprint = {1209.3353},
timestamp = {Mon, 13 Aug 2018 16:48:26 +0200},
biburl = {https://dblp.org/rec/bib/journals/corr/abs-1209-3353},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{robbins1952,
author = "Robbins, Herbert",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "09",
number = "5",
pages = "527--535",
publisher = "American Mathematical Society",
title = "Some aspects of the sequential design of experiments",
url = "https://projecteuclid.org:443/euclid.bams/1183517370",
volume = "58",
year = "1952"
}

@article{freund1997,
title = "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting",
journal = "Journal of Computer and System Sciences",
volume = "55",
number = "1",
pages = "119 - 139",
year = "1997",
issn = "0022-0000",
doi = "https://doi.org/10.1006/jcss.1997.1504",
url = "http://www.sciencedirect.com/science/article/pii/S002200009791504X",
author = "Yoav Freund and Robert E Schapire",
abstract = "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
}

@inproceedings{shen2015,
author = {Shen, Weiwei and Wang, Jun and Jiang, Yu-Gang and Zha, Hongyuan},
title = {Portfolio Choices with Orthogonal Bandit Learning},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
series = {IJCAI'15},
year = {2015},
isbn = {978-1-57735-738-4},
location = {Buenos Aires, Argentina},
pages = {974--980},
numpages = {7},
url = {http://dl.acm.org/citation.cfm?id=2832249.2832384},
acmid = {2832384},
publisher = {AAAI Press},
}